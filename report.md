# æ·±åº¦å­¸ç¿’æ‡‰ç”¨ - ä½œæ¥­äºŒ

###### tags: `ADL`

###### ç³»ç´šï¼šé›»æ©Ÿæ‰€ç¢©ä¸€ &emsp; å§“åï¼šæ¥Šå† å½¥ &emsp; å­¸è™Ÿï¼šR11921091

## Q1: Data processing

### 1. Tokenizer

#### a. Describe in detail about the tokenization algorithm you use. You need to explain what it does in your own ways.

æˆ‘ç›´æ¥ä½¿ç”¨Hugging Faceå¯«å¥½çš„codeä¾†ä½¿ç”¨Bertçš„Tokenizerã€‚ Tokenizerçš„ç›®æ¨™æ˜¯å°‡ token ç·¨ç¢¼æˆ idsï¼Œå†ç¶“é embedding layer å°‡ token è½‰ç‚ºå¸¶æœ‰è©èªæ„ç¾©çš„word vectorçµ¦æ¨¡å‹å­¸ç¿’ã€‚

`bert-base-chinese`åŠå¹¾ä¹æ‰€æœ‰ä¸­æ–‡BERTæ¨¡å‹çš„tokenizeréƒ½æ˜¯character-basedã€‚ä¹Ÿå°±æ˜¯èªªï¼Œä¸­æ–‡å­—å…ƒæœƒè¢«è¦–ä½œtokenã€‚è€Œé€™äº›Berté¡å‹çš„æ¨¡å‹ä½¿ç”¨çš„æ˜¯WordPiece Tokenizerï¼ŒGoogleè‡³ä»Šæœªé–‹æºå…¶ WordPiece è¨“ç·´ç®—æ³•çš„å¯¦ä½œç¨‹å¼ï¼Œä½†æ ¹æ“šHugging Faceçš„æ¨æ¸¬ï¼ŒWordPiece Tokenizeræœƒå…ˆå®šç¾©vocabularyå¤§å°ä»¥åŒ…å«è³‡æ–™é›†ä¸­å­˜åœ¨çš„æ¯å€‹å­—å…ƒï¼Œä¸¦å°‡wordåˆ†å‰²æˆå­—å…ƒï¼Œå†æ ¹æ“šå­—å…ƒè³‡æ–™é¸æ“‡è¦åˆä½µpairçš„æ–¹å¼ï¼Œä¸¦å»ºç«‹æ¨¡å‹ï¼Œé‡è¦†é€éé¸æ“‡èƒ½å¢åŠ æœ€å¤§æ¦‚ä¼¼çš„ subword ç›´åˆ°é”åˆ°thresholdã€‚

è£œï¼šWordPiece ä½¿ç”¨ä»¥ä¸‹å…¬å¼è¨ˆç®—æ¯pairçš„åˆ†æ•¸ï¼š
$$
score=(freq-of-pair)/(freq-of-first-elementÃ—freq-of-second-element)
$$
é€™å€‹ç®—æ³•æœƒå„ªå…ˆåˆä½µå–®å€‹éƒ¨åˆ†åœ¨volcabulary tableä¸­é »ç‡è¼ƒä½çš„pairã€‚
å…¬å¼ä¾†æºï¼š[Hugging Face WordPiece Course](https://huggingface.co/course/chapter6/6?fw=pt)

### 2. Answer Span

#### a. How did you convert the answer span start/end position on characters to position on tokens after BERT tokenization?

åœ¨ tokenizer è¨­å®š return_overflowing_tokens=True, return_offsets_mapping=Trueã€‚
return_offsets_mapping å¯ä»¥å¹«åŠ©æˆ‘å€‘å¾ character è½‰åˆ° token çš„ start endã€‚Tokenized data æœƒåŒ…å«æ¯å€‹ token å°æ‡‰ question æˆ– context çš„ (char start position, char end position)ï¼Œåªè¦è¿­ä»£æ‰¾å‡º span start èˆ‡ char start ç›¸åŒçš„ä½ç½®ä¾¿ç‚º start positionï¼Œspan end èˆ‡ char end ç›¸åŒçš„ä½ç½®å³ç‚º end positionã€‚

return_overflowing_tokens å‰‡æ˜¯ç‚ºäº†å°‡ sequence mapping å›å»ï¼Œå› ç‚ºè¼ƒé•·çš„ sequence æœƒè¢«åˆ‡æˆå¤šä»½ï¼Œé€™æ™‚å°±åªè¦å†é¡å¤–åšâ¼€äº› start end position çš„åˆ¤æ–·å°±å®Œæˆäº†ã€‚


#### b. After your model predicts the probability of answer span start/end position, what rules did you apply to determine the final start/end position?

æ¨¡å‹outputæ™‚ï¼Œéœ€è¦åˆ¤æ–· start/end position ä¸åˆç†çš„ç‹€æ³ï¼Œä»¥åŠä¸åŒçš„ start/end position çµ„åˆï¼Œåšæ³•å°±æ˜¯å°æ¯çµ„ start/end çš„é…å°æ©Ÿç‡ç›¸ä¹˜(æ²’æœ‰é exponentialçš„å‰‡ç‚ºç›¸åŠ )ã€‚å†å°‡ start position å’Œ end position çš„åˆ†æ•¸é«˜åˆ°ä½é€²è¡Œæ’åºå¾Œï¼Œæ‰¾å‡ºå…©å…©é…å°æœ€å¥½çš„å‰å¹¾åï¼Œä¸¦æ¿¾æ‰é•·åº¦ä¸åˆç†æˆ–æ˜¯ end position < start position çš„ç‹€æ³ã€‚






## Q2: Modeling with BERTs and their variants

### 1. Describe

å…ˆå˜—è©¦åŸºæœ¬çš„æ¨¡å‹Hugging Faceçš„ [bert-base-chinese](https://huggingface.co/bert-base-chinese)ã€‚

#### a. model

é¦–å…ˆæˆ‘å€‘çŸ¥é“BERT çš„input sequenceæœƒä»¥ [CLS] (question) [SEP] (context) [SEP] [PAD]...[PAD]é€™æ¨£çš„æ¨£å­æ§‹é€ ï¼Œå…¶ä¸­ [CLS] ã€ [SEP] å’Œ [PAD] æ˜¯ç‰¹æ®Šçš„tokenï¼Œ[PAD]å°‡ä¸è¶³çš„éƒ¨åˆ†è£œåˆ°æœ€å¤§sequenceé•·åº¦ï¼Œ[CLS]å’Œ[SEP]åˆ†åˆ¥æ˜¯tokenized questionå’Œcontext sequenceã€‚



##### `Context Selection`

ç°¡å–®ä¾†èªªï¼ŒBertåœ¨è¼¸å…¥sequenceé€™å€‹éƒ¨åˆ†æœƒéœ€è¦ä»¥ä¸‹å››é …ï¼š
Token embedding sequence ($\ Emb_T$)ï¼šè¨“ç·´é€™å€‹ token æœ¬èº«çš„å«ç¾©
Segment embedding sequence ($\ Emb_S$)ï¼šè¨“ç·´ä¾†åˆ†è¾¨ token æ˜¯å±¬æ–¼å“ªå€‹å¥å­ï¼Œæ‰€ä»¥åªæœ‰å…©ç¨®ä¸åŒ embedding
Position embedding sequence ($\ Emb_P$)ï¼šè¨“ç·´åˆ†è¾¨ä¸åŒ token çš„ä½ç½®
Attension mask sequence ($\ Mask_{Att}$)ï¼šå°‡æ‰€æœ‰sequenceå¡«å……åˆ°ç›¸åŒçš„é•·åº¦ï¼Œå°‡ tokens_tensors è£¡é ­ä¸ç‚º zero paddingçš„ä½ç½®è¨­ç‚º 1 è®“ BERT åªæ³¨æ„é€™äº›ä½ç½®çš„ tokensã€‚

è€Œæ ¹æ“šè€å¸«10/13ä¸Šèª²å½±ç‰‡ï¼Œå¯æ¨è«–æ¯å€‹ $\ (Emb_T, Emb_S, Emb_P, Mask_{Att})$ éƒ½è¦ç¶“éBERT encoderé€²è¡Œç‰¹å¾µæå–ã€‚ç¬¬ä¸€å€‹tokençš„æœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ï¼Œå³ [CLS] ï¼Œå…ˆç¶“ç·šæ€§å±¤è™•ç†ï¼Œå†é€éä¸€å€‹ Tanh activation functionï¼Œè¨ˆç®—å‡ºintenté¡åˆ¥çš„ logitsã€‚
ä»¥ä¸‹ç‚ºä¸Šé¢æåˆ°æ•´å€‹éç¨‹çš„å¼å­ï¼š

$\ h_{00}, h_{01}, ..., h_{N0} = Bert(Emb_{Tj}, Emb_{Sj}, Emb_{Pj}, Mask_{Attj})$
$\ y_j = Tan(Linear(h_{N0}))$

å…¶ä¸­ï¼Œ
$\ h_{it}$:è¡¨ç¤ºsequenceä¸­ç¬¬tå€‹tokençš„ç¬¬jå±¤éš±è—ç‹€æ…‹
$\ N$ï¼šæ¨¡å‹å±¤æ•¸
$\ y_j$ï¼šè¡¨ç¤ºæ˜¯æ¯å€‹å•é¡Œèˆ‡å…¶ç¬¬jå€‹contextä¹‹é–“çš„ç›¸é—œlogit

* æ¶æ§‹
  **config.json**

```json
{
  "_name_or_path": "bert-base-chinese",
  "architectures": [
    "BertForMultipleChoice"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.22.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}
```



##### `Questoin Answering`

é€™è£¡å’Œå‰é¢Multichoiceæœ‰äº›å°ä¸åŒï¼Œæ¯å€‹å•é¡Œåªæœ‰ä¸€å€‹ç›¸é—œçš„contextï¼Œä¹Ÿå°±æ˜¯è¨“ç·´æœŸé–“ä¾†è‡ªè³‡æ–™é›†çš„groundtruthï¼Œæˆ–è€…åœ¨inferenceæœŸé–“ç”±æ¨¡å‹é æ¸¬2çš„ã€‚æ¯å€‹contexté—œä¿‚çš„ $\ (Emb_T, Emb_S, Emb_P, Mask_{Att})$ ä¸€æ¨£éƒ½è¦ç¶“éBERT encoderé€²è¡Œç‰¹å¾µæå–ã€‚æ¥è‘—ï¼Œæ¯å€‹tokençš„æœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ç”±å…©å€‹ä¸åŒçš„linear layeré€²ä¸€æ­¥è™•ç†ï¼Œåˆ†åˆ¥è¨ˆç®—èµ·å§‹ä½ç½®logitåŠè¨ˆç®—ç­”æ¡ˆç¯„åœçš„çµæŸä½ç½®logitã€‚ä¹Ÿå°±æ˜¯é–‹å§‹/çµæŸä½ç½® logit è¡¨ç¤ºç‰¹å®štokenæ˜¯é–‹å§‹/çµæŸä½ç½®ç­”æ¡ˆç¯„åœçš„å¯èƒ½æ€§ã€‚
ä»¥ä¸‹ç‚ºä¸Šé¢æåˆ°æ•´å€‹éç¨‹çš„å¼å­ï¼š

$\ h_{00}, h_{01}, ..., h_{N0}, ..., h_{Nt}, ..., h_{NL-1} = Bert(Emb_{Tj}, Emb_{Sj}, Emb_{Pj}, Mask_{Attj})$
$\ starty_{t} = Tan(Linear(h_{Nt}))$
$\ endy_{t} = Tan(Linear(h_{Nt}))$

å…¶ä¸­ï¼Œ
$\ h_{it}$:è¡¨ç¤ºsequenceä¸­ç¬¬tå€‹tokençš„ç¬¬jå±¤éš±è—ç‹€æ…‹
$\ N$ï¼šæ¨¡å‹å±¤æ•¸
$\ starty_{j}$ï¼štoken tçš„èµ·å§‹ä½ç½®logit
$\ endy_{t}$ï¼štoken tçš„çµæŸä½ç½®logit
$\ L$ï¼šè¡¨æœ€å¤§sequenceé•·åº¦

* æ¶æ§‹
  **config.json**

```json
{
  "_name_or_path": "bert-base-chinese",
  "architectures": [
    "BertForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.22.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}
```



#### b. Performance

* Context selection accuracy: 0.95314
* Question answering EM:  0.79561
* Question answering F1: 0.86168
* Public accuracy on Kaggle: 0.74231
* Private accuracy on Kaggle: 0.76242



#### c. Loss function

Context Selection å’Œ Questoin Answeringæˆ‘éƒ½ä½¿ç”¨æ¨™æº–[Cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
$\ Loss = CrossEntropyLoss(y*, ğ‘”ğ‘¡)$ï¼Œ$\ y*$ç‚º intent classifier outputï¼Œ$\ gt$ ç‚º ground truth ã€‚



#### d. Optimization algorithm, learning rate and batch size etc.

##### `Context Selection`

* Optimization algorithm: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)
* Learning rate: 3e-5 = 0.00003
* Batch_size: 2 (per_gpu_train_batch_size 1 * gradient_accumulation_steps 2)
* Num_train_epochs: 3
* Max_len: 512

##### `Questoin Answering`

* Optimization algorithm: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)

* Learning rate: 3e-5 = 0.00003

* Batch_size: 2 (per_gpu_train_batch_size 1 * gradient_accumulation_steps 2)

* Num_train_epochs: 1

* Max_len: 512

  

### 2. Try another type of pretrained model and describe

åƒè€ƒè€å¸«10/20çš„ä¸Šèª²å½±ç‰‡ï¼Œæˆ‘å˜—è©¦äº†Hugging Faceçš„ [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext) ã€[hfl/chinese-roberta-wwm-ext-large](https://huggingface.co/hfl/chinese-roberta-wwm-ext-large) ã€[hfl/chinese-macbert-base](https://huggingface.co/hfl/chinese-macbert-base) ã€[hfl/chinese-macbert-large](https://huggingface.co/hfl/chinese-macbert-large) ã€ [hfl/chinese-pert-base](https://huggingface.co/hfl/chinese-pert-base) ã€ [hfl/chinese-lert-base](https://huggingface.co/hfl/chinese-lert-base) ã€[hfl/chinese-lert-large](https://huggingface.co/hfl/chinese-lert-large) çš„æ¨¡å‹ï¼Œæœ€å¾Œé¸æ“‡åœ¨Context Selectionä½¿ç”¨[hfl/chinese-macbert-large](https://huggingface.co/hfl/chinese-macbert-large) ï¼›Question Answeringä½¿ç”¨[hfl/chinese-lert-large](https://huggingface.co/hfl/chinese-lert-large) ï¼Œå¦‚å°å‰è¿°æˆ‘å˜—è©¦éæ¨¡å‹çš„Performaceæœ‰èˆˆè¶£ï¼Œå¯æŸ¥çœ‹æœ¬ä»½æ–‡ä»¶æœ€ä¸‹æ–¹çš„ **Experiment Result **ï¼Œæˆ‘å°‡Performaceå’Œåƒæ•¸é…ç½®ç°¡å–®æ•´ç†åœ¨é‚£è£¡ã€‚



#### a. model

##### `Context Selection`

æ¯å€‹ $\ (Emb_T, Emb_S, Emb_P, Mask_{Att})$ éƒ½è¦ç¶“éBERT encoderé€²è¡Œç‰¹å¾µæå–ã€‚ç¬¬ä¸€å€‹tokençš„æœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ï¼Œå³ [CLS] ï¼Œå…ˆç¶“ç·šæ€§å±¤è™•ç†ï¼Œå†é€éä¸€å€‹ Tanh activation functionï¼Œè¨ˆç®—å‡ºintenté¡åˆ¥çš„ logitsã€‚
ä»¥ä¸‹ç‚ºä¸Šé¢æåˆ°æ•´å€‹éç¨‹çš„å¼å­ï¼š

$\ h_{00}, h_{01}, ..., h_{N0} = Chinese-Macbert-large(Emb_{Tj}, Emb_{Sj}, Emb_{Pj}, Mask_{Attj})$
$\ y_j = Tan(Linear(h_{N0}))$

å…¶ä¸­ï¼Œ
$\ h_{it}$:è¡¨ç¤ºsequenceä¸­ç¬¬tå€‹tokençš„ç¬¬jå±¤éš±è—ç‹€æ…‹
$\ N$ï¼šæ¨¡å‹å±¤æ•¸
$\ y_j$ï¼šè¡¨ç¤ºæ˜¯æ¯å€‹å•é¡Œèˆ‡å…¶ç¬¬jå€‹contextä¹‹é–“çš„ç›¸é—œlogit

* æ¶æ§‹

**config.json**

```json
{
  "_name_or_path": "hfl/chinese-roberta-wwm-ext",
  "architectures": [
    "BertForMultipleChoice"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "directionality": "bidi",
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.22.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}
```



##### `Questoin Answering`

é€™è£¡æˆ‘ä½¿ç”¨äº†Hugging Faceçš„ [hfl/chinese-lert-large](https://huggingface.co/hfl/chinese-lert-large) ï¼Œé€™è£¡å’ŒQ2ä¸€æ¨£ï¼Œæ¯å€‹å•é¡Œåªæœ‰ä¸€å€‹ç›¸é—œçš„contextï¼Œä¹Ÿå°±æ˜¯è¨“ç·´æœŸé–“ä¾†è‡ªè³‡æ–™é›†çš„groundtruthï¼Œæˆ–è€…åœ¨inferenceæœŸé–“ç”±æ¨¡å‹é æ¸¬2çš„ã€‚æ¯å€‹contexté—œä¿‚çš„ $\ (Emb_T, Emb_S, Emb_P, Mask_{Att})$ ä¸€æ¨£éƒ½è¦ç¶“éBERT encoderé€²è¡Œç‰¹å¾µæå–ã€‚æ¥è‘—ï¼Œæ¯å€‹tokençš„æœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ç”±å…©å€‹ä¸åŒçš„cosine layeré€²ä¸€æ­¥è™•ç†ï¼Œåˆ†åˆ¥è¨ˆç®—èµ·å§‹ä½ç½®logitåŠè¨ˆç®—ç­”æ¡ˆç¯„åœçš„çµæŸä½ç½®logitã€‚ä¹Ÿå°±æ˜¯é–‹å§‹/çµæŸä½ç½® logit è¡¨ç¤ºç‰¹å®štokenæ˜¯é–‹å§‹/çµæŸä½ç½®ç­”æ¡ˆç¯„åœçš„å¯èƒ½æ€§ã€‚
ä»¥ä¸‹ç‚ºä¸Šé¢æåˆ°æ•´å€‹éç¨‹çš„å¼å­ï¼š

$\ h_{00}, h_{01}, ..., h_{N0}, ..., h_{Nt}, ..., h_{NL-1} = Chinese-Lert-Large(Emb_{Tj}, Emb_{Sj}, Emb_{Pj}, Mask_{Attj})$
$\ starty_{t} = Tan(Linear(h_{Nt}))$
$\ endy_{t} = Tan(Linear(h_{Nt}))$

å…¶ä¸­ï¼Œ
$\ h_{it}$:è¡¨ç¤ºsequenceä¸­ç¬¬tå€‹tokençš„ç¬¬jå±¤éš±è—ç‹€æ…‹
$\ N$ï¼šæ¨¡å‹å±¤æ•¸
$\ starty_{j}$ï¼štoken tçš„èµ·å§‹ä½ç½®logit
$\ endy_{t}$ï¼štoken tçš„çµæŸä½ç½®logit
$\ L$ï¼šè¡¨æœ€å¤§sequenceé•·åº¦

* æ¶æ§‹

**config.json**

```json
{
  "_name_or_path": "hfl/chinese-lert-large",
  "architectures": [
    "BertForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "pooler_fc_size": 1024,
  "pooler_num_attention_heads": 16,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.22.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}
```



#### b. Performance

* Context selection accuracy: 0.96610
* Question answering EM: 0.84978
* Question answering F1: 0.91329
* Public accuracy on Kaggle: 0.80831
* Private accuracy on Kaggle: 0.81752



#### c. Loss function

Context Selectionå’ŒQuestoin Answeringæˆ‘éƒ½ä½¿ç”¨æ¨™æº–[Cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
$\ Loss = CrossEntropyLoss(y*, ğ‘”ğ‘¡)$ï¼Œ$\ y*$ç‚º intent classifier outputï¼Œ$\ gt$ ç‚º ground truth ã€‚



#### d. Optimization algorithm, learning rate and batch size etc.

##### Context Selection

* Optimization algorithm: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)
* Learning rate: 3e-5 = 0.00003
* Batch_size: 8 (per_gpu_train_batch_size 1 * gradient_accumulation_steps 2)
* Num_train_epochs: 2
* Max_len: 512

##### Questoin Answering

* Optimization algorithm: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)
* Learning rate: 3e-5 = 0.00003
* Batch_size: 64 (per_gpu_train_batch_size 1 * gradient_accumulation_steps 8)
* Num_train_epochs: 6
* Max_len: 512











## Q3: Plot the learning curve of my QA model

### a. Learning curve of loss 

![image-20221108164956299](C:\Users\wealt\AppData\Roaming\Typora\typora-user-images\image-20221108164956299.png)

### b. Learning curve of EM&F1

![image-20221110010255084](C:\Users\wealt\AppData\Roaming\Typora\typora-user-images\image-20221110010255084.png)



## Q4: Pretrained vs Not Pretrained

é€™éƒ¨åˆ†æˆ‘å°‡Quetion Answeringéƒ¨åˆ†è·‘Not Pretrainedçš„ç‹€æ³ï¼Œä¸¦èˆ‡Pretrainedç‹€æ³æ¯”è¼ƒã€‚
å…¶å¯¦å°±æ˜¯AutoModelForQuestionAnswering.from_pretrainedè¨»è§£æ‰æ”¹æˆAutoModelForQuestionAnswering.from_configï¼Œå¦‚ä¸‹ï¼š

**run_qa_no_trainer.py**

```python
    config = AutoConfig.from_pretrained(args.model_name_or_path)
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)
    
    # Pretrained
    # model = AutoModelForQuestionAnswering.from_pretrained(
    #             args.model_name_or_path,
    #             from_tf=bool(".ckpt" in args.model_name_or_path),
    #             config=config,
    #         )
    
    # Not Pretrained
    model = AutoModelForQuestionAnswering.from_config(config)
```



#### a. Not Pretrained architecture

hidden_size: 768 $\rightarrow$ 32

num_attention_heads: 12 $\rightarrow$ 4

num_hidden_layers: 12 $\rightarrow$ 4

* æ¶æ§‹

**config.json**

```json
{
  "_name_or_path": "bert-base-chinese",
  "architectures": [
    "BertForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 32,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 4,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.22.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}
```



#### b. Loss function

èˆ‡ç¬¬2é¡Œæè¿°çš„Loss functionç›¸åŒ
Questoin Answeringæˆ‘éƒ½ä½¿ç”¨æ¨™æº–[Cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
$\ Loss = CrossEntropyLoss(y*, ğ‘”ğ‘¡)$ï¼Œ$\ y*$ç‚º intent classifier outputï¼Œ$\ gt$ ç‚º ground truth ã€‚



#### c. Optimization algorithm, learning rate and batch size etc.

èˆ‡ç¬¬2é¡Œæè¿°çš„åƒæ•¸éƒ½ç›¸åŒ

* Optimization algorithm: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)
* Learning rate: 3e-5 = 0.00003
* Batch_size: 2 (per_gpu_train_batch_size 1 * gradient_accumulation_steps 2)
* Num_train_epochs: 1
* Max_len: 512



#### d. Compare the performance with Pretrained

| Qustion Answering | Context Selection | Public EM | Public F1 |
| ----------------- | ----------------- | --------- | --------- |
| bert-base-chinese | bert-base-chinese | 0.79561   | 0.86168   |
| Not Pretrained    | bert-base-chinese | 0.03755   | 0.0707    |

æ˜é¡¯å¯ä»¥è¦‹åˆ°æº–ç¢ºåº¦å·®ç•°éå¸¸å¤§ï¼Œå³ä½¿æˆ‘æŠŠQustion Answering Not Prtrainedçš„æ¨¡å‹num_train_epochså†æ”¹æˆ10ï¼ŒPublic EMä¹Ÿåƒ…åˆ°0.04320ï¼Œé€™æ˜¯ç”±æ–¼ Trainsformer æ¶æ§‹é¾å¤§ï¼Œéœ€è¦å¤§é‡è¨“ç·´è³‡æ–™èˆ‡æ™‚é–“æ‰èƒ½è¨“ç·´å¾—èµ·ä¾†ï¼Œè€Œæœ¬æ¬¡è¨“ç·´è³‡æ–™é›†ç›¸å°éå¸¸å°ï¼Œå› æ­¤å®¹æ˜“è¼ƒå¿«å‡ºç¾overfittingçš„ç‹€æ³ï¼Œè€Œä¸”æˆ‘å€‘ä¹Ÿæ²’æœ‰è¶³å¤ ç®—åŠ›æˆ–æ˜¯æ™‚é–“ä¾†é‡é ­è¨“ç·´Bertæ¨¡å‹ã€‚













## Q5: Bonus: HW1 with BERTs

#### a. model


##### `Intent Classification`

é€™è£¡ä½¿ç”¨ [bert-base-uncased](https://huggingface.co/bert-base-uncased) pretrained model æ•´å€‹éç¨‹èˆ‡ç¬¬äºŒé¡Œçš„context selectionå·®ä¸å¤šï¼Œå¯ç”¨ä»¥ä¸‹å¼å­æè¿°ï¼š

$\ h_{00}, h_{01}, ..., h_{N0} = Bert(Emb_{Tj}, Emb_{Sj}, Emb_{Pj}, Mask_{Attj})$
$\ y(0), . . . , y(149) = Tan(Linear(h_{N0}))$



##### `Slot tagging`

é€™è£¡ä½¿ç”¨ [bert-base-uncased](https://huggingface.co/bert-base-uncased) pretrained model æ•´å€‹éç¨‹èˆ‡ç¬¬äºŒé¡Œçš„question answeringå·®ä¸å¤šï¼Œå¯ç”¨ä»¥ä¸‹å¼å­æè¿°ï¼š

$\ h_{00}, h_{01}, ..., h_{N0}, ..., h_{Nt}, ..., h_{NL-1} = Bert(Emb_{Tj}, Emb_{Sj}, Emb_{Pj}, Mask_{Attj})$
$\ y(t0), . . . , y(t9) = Tan(Linear(h_{Nt}))$



#### b. Loss function

Context Selectionå’ŒQuestoin Answeringæˆ‘éƒ½ä½¿ç”¨æ¨™æº–[Cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
$\ Loss = CrossEntropyLoss(y*, ğ‘”ğ‘¡)$ï¼Œ$\ y*$ç‚º intent classifier outputï¼Œ$\ gt$ ç‚º ground truth ã€‚



#### c. Optimization algorithm, learning rate and batch size etc.

##### Intent Classification

* Optimization algorithm: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)
* Learning rate: 1e-5 = 0.00001
* Batch_size: 32
* Num_train_epochs: 8
* Max_len: 128

##### Slot tagging 

* Optimization algorithm: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)
* Learning rate: 1e-5 = 0.00001
* Batch_size: 32
* Num_train_epochs: 8
* Max_len: 128



#### d. Performace & Compare with my final model in HW1

##### `Intent Classification`

* HW1

| Name | Number of<br>Layer | Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :--: | :----------------: | -------------- | ------------- | ------------- | ------------- | ------------ | ------------ |
| LSTM |  *2<sup>*</sup>*   | 256            | 70            | 0.0502        | 0.9996        | 0.72006      | 0.92         |

* Comparing Table

|       Name        | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. | Public Acc.<br>on Kaggle | Private Acc.<br>on Kaggle |
| :---------------: | :-----------: | ------------- | ------------ | ------------ | ------------------------ | ------------------------- |
|       LSTM        |    0.0502     | 0.9996        | 0.72006      | 0.92         | 0.91511                  | 0.91466                   |
| bert-base-uncased |    0.19185    | 0.99667       | 0.24433      | 0.967333     | 0.96133                  | 0.96088                   |



##### `Slot tagging`

* HW1

| Name | Number of<br>Layer | Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :--: | :----------------: | -------------- | ------------- | ------------- | ------------- | ------------ | ------------ |
| GRU  |  *2<sup>*</sup>*   | 512            | 100           | 0.002134      | 0.947404      | 0.017795     | 0.841        |


* Comparing Table

|       Name        | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. | Public Acc.<br>on Kaggle | Private Acc.<br>on Kaggle |
| :---------------: | :-----------: | ------------- | ------------ | ------------ | ------------------------ | ------------------------- |
|        GRU        |   0.002134    | 0.947404      | 0.017795     | 0.841        | 0.83217                  | 0.81511                   |
| bert-base-uncased |    0.00581    | 0.80094       | 0.00683      | 0.752        | 0.74852                  | 0.7717                    |



















## Experiment Result

| Model Name (CS)                   | Model Name (QA)             | Gradient Accumulation Steps (MC) | Gradient Accumulation Steps (QA) | Num Train Epochs (CS) | Num Train Epochs (QA) | lr scheduler type (CS) | lr scheduler type (QA) | Public Acc.<br/>on Kaggle |
| --------------------------------- | --------------------------- | -------------------------------- | -------------------------------- | --------------------- | --------------------- | ---------------------- | ---------------------- | ------------------------- |
| bert-base-chinese                 | bert-base-chinese           | 2                                | 2                                | 3                     | 1                     | linear                 | linear                 | 0.74231                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-roberta-wwm-ext | 2                                | 2                                | 3                     | 3                     | linear                 | linear                 | 0.76401                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-roberta-wwm-ext | 2                                | 2                                | 3                     | 3                     | linear                 | cosine                 | 0.783                     |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-roberta-wwm-ext | 2                                | 2                                | 3                     | 20                    | linear                 | cosine                 | 0.77215                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-roberta-wwm-ext | 2                                | 2                                | 3                     | 20                    | linear                 | linear                 | 0.7613                    |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-pert-base       | 2                                | 8                                | 3                     | 10                    | linear                 | linear                 | 0.76672                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-lert-base       | 2                                | 8                                | 3                     | 10                    | linear                 | cosine                 | 0.79023                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-lert-base       | 2                                | 8                                | 3                     | 10                    | linear                 | linear                 | 0.79023                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-lert-base       | 2                                | 8                                | 3                     | 15                    | linear                 | linear                 | 0.79023                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-lert-base       | 2                                | 8                                | 3                     | 20                    | linear                 | linear                 | 0.78119                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-lert-base       | 2                                | 8                                | 5                     | 10                    | linear                 | linear                 | 0.78661                   |
| hfl/chinese-roberta-wwm-ext       | hfl/chinese-lert-large      | 2                                | 8                                | 3                     | 6                     | linear                 | linear                 | 0.79475                   |
| hfl/chinese-roberta-wwm-ext-large | hfl/chinese-lert-large      | 8                                | 8                                | 2                     | 6                     | linear                 | linear                 | 0.79385                   |
| hfl/chinese-macbert-base          | hfl/chinese-lert-large      | 8                                | 8                                | 6                     | 6                     | linear                 | linear                 | 0.79927                   |
| hfl/chinese-macbert-large         | hfl/chinese-lert-large      | 8                                | 8                                | 2                     | 6                     | linear                 | linear                 | 0.80831                   |
| hfl/chinese-macbert-large         | hfl/chinese-lert-large      | 8                                | 64                               | 2                     | 10                    | linear                 | linear                 | 0.80289                   |

CS: Context Selection

QA: Question Answering







## Appendix

é‡å°lertæ¨¡å‹å·²æ–¼githubè©¢å•å“ˆå·¥å¤§è¨Šé£›è¯åˆå¯¦é©—å®¤ï¼Œç¢ºèªéQA/NLIæ¨¡å‹ã€‚

![image-20221110004924366](C:\Users\wealt\AppData\Roaming\Typora\typora-user-images\image-20221110004924366.png)



## Reference

* [LeeMeng - é€²æ“Šçš„ BERTï¼šNLP ç•Œçš„å·¨äººä¹‹åŠ›èˆ‡é·ç§»å­¸ç¿’](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)
* [Transformer github](https://github.com/huggingface/transformers/tree/v4.22.0/examples/pytorch)
* [Hugging Face](https://huggingface.co)
* [Revisiting Pre-Trained Models for {C}hinese Natural Language Processing](https://arxiv.org/pdf/2004.13922)
* [HFL Github](https://github.com/iflytek/HFL-Anthology)

